{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from google.cloud import storage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='BM-K/KoSimCSE-roberta-multitask'\n",
    "BUCKET_NAME='law-search'  # ==> GCS 저장소 bucket name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_embedding(sub_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Text 데이터 embedding 데이터 변환\n",
    "    \"\"\"\n",
    "    # 컬럼 변수\n",
    "    col_embedding='embedding'\n",
    "\n",
    "    # 모델 & 토크나이저\n",
    "    model=AutoModel.from_pretrained(MODEL_NAME)\n",
    "    tokenizer=AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    embedding_list=[]\n",
    "    for _, row in sub_df.iterrows():\n",
    "        law_text=row.law\n",
    "        \n",
    "        # 전처리 pattern 및 전처리 진행\n",
    "        pattern=r'\\<(.*?)\\>'\n",
    "        prepro_text=re.sub(pattern, '', law_text).strip()\n",
    "\n",
    "        # 특수기호 제거 및 띄어쓰기 반복 변형\n",
    "        prepro_text=re.sub(r' +', ' ',re.sub(r'[^가-힣a-zA-Z0-9]', ' ', prepro_text)).strip()\n",
    "\n",
    "        # Tokenize sentences\n",
    "        encoded_input = tokenizer(prepro_text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        \n",
    "        # Perform pooling. In this case, mean pooling.\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        # 데이터 정립\n",
    "        embedding_list.append(sentence_embeddings[0].tolist())\n",
    "\n",
    "    prepro_sub_df=sub_df.copy()\n",
    "    prepro_sub_df[col_embedding]=embedding_list\n",
    "    \n",
    "    return prepro_sub_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
