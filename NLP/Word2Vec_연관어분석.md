# Word2Vec

> - 참고 : Word2vec 모델은 자연어 처리에서 단어를 밀집 벡터로 만들어주는 단어 임베딩 방법론이지만 최근에 들어서는 자연어 처리를 넘어서 추천 시스템에도 사용되고 있는 모델입니다. 우선 적당하게 데이터를 나열해주면 Word2vec은 위치가 근접한 데이터를 유사도가 높은 벡터를 만들어준다는 점에서 착안된 아이디어입니다.
> - 참고할만한 유용한 사이트 : https://wikidocs.net/50739
> - 입력데이터는 아래와 같은 형식으로 들어가야 함.
> - [['영문', '작성', '시', '도전', '통해', '발전', '삶', '산업혁명', '도래', '인해', '인공', ...],
>    ['최근', '사회', '이슈', '중', '중요하다고', '생각', '되는', '한가지', '선택', 자신'...]]
> - 하나의 list 데이터가 하나의 문서(document)

```python
# 데이터 정리
new_texts = [x for x in analysis_data.values ]
```



### Word2Vec 파라미터

> - sg=0 : CBOW / sg=1 : Skip-gram.
> - window=2 : 중심단어로부터 좌우 2개 단어까지 학습 적용
> - min_count=3 : 전체 문서에서 최소 3번 이상 출현한 단어들로 학습 진행

```python
from gensim.model.word2vec import Word2Vec
model = Word2Vec(new_texts, sg=1, window=2, min_count=3)
```



> - Word2Vec 임베딩 행렬 크기
> - (9807, 100) ==> 총 9,807개의 단어가 존재하며 각 단어는 100차원의 벡터로 구성!

```python
model.wv.vectors.shape
```



> - 두 단어간 유사도 확인

```python
model.similarity('노력', '직무') # ==> 0.4238115
model.similarity('성과', '운')  # ==> 0.5904772
model.similarity('데이터', '지능') # ==> 0.56477046
```



> - 한 단어의 벡터값 확인

```python
model['우수'] # ==> 100개의 벡터값이 나옴
'''
array([-0.002539  , -0.06057164, -0.21171942, -0.1135043 , -0.02038069,
       -0.11256956,  0.02739452, -0.14090475,  0.03007646,  0.23465586,
       -0.17316961,  0.41919255, -0.09107862,  0.09444676, -0.05774161,
        0.1845567 ,  0.19271594, -0.06102462, -0.15735976,  0.11052014,
       -0.07309107, -0.22131294, -0.257398  , -0.01043399,  0.18507005,
       -0.08730992, -0.00072695,  0.09098671,  0.03224645, -0.09390066,
        0.15575832, -0.02193078, -0.21297002,  0.05585172, -0.13468964,
        0.2613472 , -0.27466497,  0.16083005, -0.22486125, -0.11347666,
       -0.07764699, -0.05525779, -0.02436115,  0.18342403, -0.07581959,
       -0.29773   , -0.03689386,  0.24037196,  0.1358245 , -0.13924715,
        0.06381249,  0.31632203, -0.22355069, -0.00261679, -0.2361036 ,
       -0.1768923 ,  0.19399948,  0.10332637, -0.17251067, -0.17183083,
        0.22672218,  0.20263378, -0.04204553, -0.03017035,  0.27352262,
        0.17141138, -0.02135423,  0.2246755 ,  0.00305618, -0.00082171,
       -0.25556603,  0.07341506, -0.18579431,  0.41067994, -0.0748823 ,
        0.10808787,  0.20867613,  0.41281947, -0.11488044, -0.06532883,
        0.13860685,  0.05028524,  0.10803444, -0.25147822, -0.16613531,
        0.06902415,  0.19052754,  0.03202201,  0.08208697, -0.13784139,
        0.0311874 ,  0.5550599 ,  0.2274637 ,  0.28824863, -0.0348101 ,
        0.2788993 , -0.23658967,  0.35295725,  0.00658568,  0.02867782],
      dtype=float32)
'''
```



> -  성공이란 단어와 연관 높은 단어들

```python
model.wv.most_similar('성공')
'''
[('이루어', 0.8150209784507751),
 ('냈습니다', 0.7964283227920532),
 ('마쳤습니다', 0.78409743309021),
 ('이끌어', 0.7834421396255493),
 ('내고', 0.7714000940322876),
 ('장기간', 0.7700888514518738),
 ('최종', 0.7693543434143066),
 ('이뤄', 0.7674574255943298),
 ('공동', 0.7673068046569824),
 ('해보거나', 0.7667516469955444)]
'''
```

