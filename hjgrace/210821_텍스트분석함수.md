## 라이브러리

```python
import numpy as np
import pandas as pd
import re
from konlpy.tag import Okt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
import matplotlib.pyplot as plt
from matplotlib import font_manager, rc
from wordcloud import WordCloud
from collections import Counter
```



## 그래프 폰트 설정

```python
# 폰트 사이즈
plt.rcParams['font.size'] = 25
# 폰트 설정
plt.rcParams['font.family'] = 'Malgun Gothic'
# 화면크기 설정
plt.rcParams['figure.figsize'] = (16,18)
```



## 함수 모음

### 나이 분리 함수

```python
# 나이대 분리 함수
def age_split(age):
    if age < 20 :
        result = 1
    elif age < 30 :
        result = 2
    elif age < 40 :
        result = 3
    elif age < 50 :
        result = 4
    elif age < 60 :
        result = 5
    else:
        result = 6
        
    return result
```



### 지역 분리 함수

```python
# 지역 분리 함수
def location_split(location):
    if location in [1,4,8]:
        result = 1 # ==> 수도권
    elif location in [6, 10, 11, 17]:
        result = 2 # 충청권
    elif location in [5, 12, 13]:
        result = 3 # 전라권
    elif location in [2, 3, 7, 14, 15]:
        result = 4 # 경상권
    elif location in [9]:
        result = 5 # 강원권
    else:
        result = 6 # 제주
    return result
```



## 형태소 분석 Tokenize

```python
# okt 형태소 분석기 토크나이징
def tokenize(text):
    okt = Okt()
    tokens = okt.pos(text)

#     stop_words = ['있는', '하는', '생각', '합니다', '대한', '필요', '먼저'
#              '한다', '마음', '문제', '대통령', '위해', '않는', '의견', '나라']
#     tokens = [(word, tag) for word, tag in tokens if word not in stop_words]
    
    total_words = []
    for word, tag in tokens:
        if tag not in ['Josa', 'Suffix']:
            total_words.append(word)
    result = ' '.join(total_words)
    return result
```



## 빈도분석 함수

```python
# 빈도분석 함수
def count_vectorize(text, vectorizer):
    word_dict = sorted(vectorizer.vocabulary_.items())
    idx2word = {idx:word for word, idx in word_dict}
    
    total_word = []
    total_word.append((' ').join(text.values))
    
    count_matrix = count_vectorizer.transform(total_word)
    
    count_word = []
    count_vector = []
    
    for i in range(20,0,-1):
        count_word.append(idx2word[(-count_matrix.toarray()[0]).argsort()[i-1]])
        count_vector.append(count_matrix.toarray()[0][(-count_matrix.toarray()[0]).argsort()[i-1]]) 
    
    return count_word, count_vector
```



## TF-IDF 분석 함수

```python
# tf-idf 분석
def tfidf_data(text_data, stop_word, tfidf):
    _tokeniz = text_data.map(tokenize)
    
    _word = []
    for sentence in [text for text in _tokeniz.values]:
        for word in sentence.split(' '):
            if word not in stop_word:
                _word.append(word)
                
    _total_word = []
    _total_word.append(' '.join(_word))

    _tfidf_matrix = tfidf.transform(_total_word)
    
    # 단어사전 정렬
    word_dict = sorted(tfidf.vocabulary_.items())
    idx2word = {idx:word for word, idx in word_dict}
    
    _tfidf_word = []
    _tfidf = []
    for i in range(25,0,-1):
        _tfidf_word.append(idx2word[(-_tfidf_matrix.toarray()[0]).argsort()[i-1]])
        _tfidf.append(_tfidf_matrix.toarray()[0][(-_tfidf_matrix.toarray()[0]).argsort()[i-1]])

    return _tfidf_word, _tfidf
```



## 가로 막대 그래프 그리기

```python
# 가로 막대 그래프 그리기
def count_graph(word, vector, color, title):
    plt.barh(word, vector, label='단어 빈도', color=color)
    plt.ylabel('단어')
    plt.xlabel('빈도')
    plt.legend()
    plt.yticks(word)
    plt.title('{} 텍스트 빈도 분석 Top20'.format(title))
    plt.tight_layout()
    plt.savefig('{}.png'.format(title))
    plt.show()
```



## 불용어 사전

```python
# 불용어 사전은 빈도와 tfidf 등 vetor로 바꾼 후 높은 숫자를 가진 단어들 중에서 필요없는 단어들을 세팅한다.
# 각 데이터마타 불용어 사전은 다르게 나타날 수 밖에 없다
stop_words = ['대한', '있도록', '있는', '하여', '하는', '있음', '위해', '대해', '항상',
              '합니다', '로서', '업무', '하십니다', '같습니다', '없습니다', '경우', '없음',
              '있습니다', '가지', '너무', '바탕', '모습', '생각', '좋겠습니다', '부분', '본인',
              '하며', '있어', '부문', '하지']
```





## Count Vectorizer

```python
# max_features == 전체 텍스트에서 빈도수를 파악할 단어를 빈도순대로 쭉 나열했을 때 세팅한 숫자만큼 구성한다.
# max_df == 전체 문서에서 30% 정도 차지하는 빈도를 최대 빈도로 제한한다.(한국어는 조사, 복수표현 등 의미 없는 형태소가 많이 쓰이고, 이렇게 많이 쓰인 텍스트를 제한해야 이 문서에서 의미 있게 쓰인 단어들이 무엇인지 알 수 있다)
# min_df == max_df 와 반대로 만약 100이란 숫자를 세팅했으면 전체 문서에서 100번 이상 빈도를 가진 단어만 count_vetorizer의 단어사전으로 포함시킨다.
# ngram_range == default가 (1,1)이다 즉, 1단어만 찾겠다는 의미고, (1,2) 1개 ~ 2개 단어 중에서 빈도 높은 텍스트를 찾겠다는 의미고, (2,2)는 2개 단어 중에서 빈도 높은 단어들을 찾겠다는 의미다.
# stop_words == 위에서 만든 불용어사전을 넣어주면 모델을 구성할 때, 자동으로 불용어의 단어들은 제거하고 분석해준다.
count_vec = CountVectorizer(max_features = 200,
                            #max_df = 0.3,
                            min_df = 100,
                            ngram_range = (2,2)
                            stop_words = stop_words)
```





## TF-IDF 생성

```python
# tfidf 하이퍼파라미터 설정
# tfidf도 위 count vectorizer의 파라미터와 동일한 의미를 가진다.
tfidf = TfidfVectorizer(stop_words=stop_words,
                        min_df = 100,
                        max_features = 500)
```

