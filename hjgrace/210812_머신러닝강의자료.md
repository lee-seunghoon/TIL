## Linear Regression Model (선형 회귀 모델)

> - 어떠한 `데이터`에 대해서 그 값에 영향을 주는 `조건(회귀계수)`을 고려하여 데이터의 `평균`을 구하기 위한 함수
> - 평균이라는 표현은 어떤 의미?? >>> 평균적으로' 라는 표현은 `대표성`을 나타낸다. (But, 극단의 이상치가 존재하면 대표성이 깨진다.)
> - 그래서 Regression Model(회귀모델) 다시 정의 ==> 어떠한 `데이터`에 대해서 그 값에 영향을 주는 `조건(회귀계수)`을 고려하여 그 데이터를 `가장 잘 표현하는 함수`
> - 독립변수가 1개인 함수를 가정 ==> `Υ = β0 + β¹x` ==> Y = ax + b (직선!) ==> `β0` : 기타 영향을 주는 요인 / `β¹` : x에 영향을 주는 요인
> - 결론 : Regression Model은 주어진 데이터를 가장 잘 표현하는 `직선`을 찾는 것으로 정의할 수 있다.



### 진짜 쉽게 식으로 표현

> - y = Wx + b 
> - `W`:학습 가중치
> - `b`:bias
> - 초기 W와 b의 값은 랜덤 값



### `sklearn` / simple linear regression 구현

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model

#1. training data set
x_data = np.array([1,2,3,4,5]).reshape(5,1)
t_data = np.array([3,5,7,9,11]).reshape(5,1)

# 산점도 그려보기
plt.scatter(x_data, t_data)
plt.xticks(np.arange(1,6,1))
plt.show()

#2. 모델 만들어보기
model = linear_model.LinearRegression()

#3. 학습
# model.fit(입력값, 레이블값)
# model.fit(독립변수, 종속변수)
model.fit(x_data,t_data)

#4. Weight와 bias 확인해보자
print('W:',model.coef_, 'b:', model.intercept_)

#5. predict
print(model.predict([[9]]))
```



### Ozone data 실습

> - 단순 선형 회귀 모델

```python
import numpy as np
import pandas as pd
from sklearn import linear_model
import matplotlib.pyplot as plt
from scipy import stats # ==> 이상치 제거

##################################################################
# 데이터 탐색
df = pd.read_csv('/content/drive/MyDrive/Google Colab/ozone.csv')

# 독립변수 : Ozone / 종속변수 : Temp
new_df = df[['Ozone','Temp']]

# null 값 확인
new_df.info()

# 전체 통계수치 간단히 확인
new_df.describe()

# 전체 Nan 개수 확인
new_df.isna().sum()

##################################################################

# 데이터 전처리
# Nan 삭제
new_df = new_df.dropna(axis=0, how='any')
new_df

# 결측치 채우기
new_df['Ozone'] = new_df['Ozone'].fillna(int(new_df['Ozone'].mean()))

# boxplot
plt.boxplot(new_df['Ozone'],showmeans=True)
plt.show()

# 이상치 제거
# zscore 임계치
zscore_threshold = 1.9
outlier_remove = ~(np.abs(stats.zscore(new_df['Ozone']))>zscore_threshold)
new_df = new_df[outlier_remove]

##################################################################
# 독립변수 데이터셋, 종속변수 데이터셋 구축
x_data = new_df['Temp'].values.reshape(-1,1)
t_data = new_df['Ozone'].values.reshape(-1,1)

##################################################################
# 모델 생성
model = linear_model.LinearRegression() # ==> 선형회귀 모델 생성

##################################################################
# 학습(fit)
model.fit(x_data, t_data)

##################################################################
# W와 b 값 확인
print('W:', model.coef_, 'b:', model.intercept_)
# ==> W: [[2.4287033]] b: [-146.99549097]

##################################################################
# predict
prediction = model.predict([[85]])
print(prediction)

##################################################################
# 그래프 그리기
plt.scatter(x_data, t_data)
plt.plot(x_data, (model.coef_ * x_data) + model.intercept_, color='red')
plt.show()

```



> - 다중 선형 회귀 모델

```python
import numpy as np
import pandas as pd
from scipy import stats # ==> zscore 이상치 제거시 사용
from sklearn import linear_model

from sklearn.preprocessing import MinMaxScaler, StandardScaler
# ==> Min Max Normalization 정규화 작업 시 사용


# 데이터 불러오기
df = pd.read_csv('/content/drive/MyDrive/Google Colab/ozone.csv')
training_data = df[['Temp', 'Wind', 'Solar.R', 'Ozone']]

# 데이터 탐색
training_data.info()
training_data.describe()

# 결측치 개수 확인
training_data.isna().sum()

# 결측치 처리 (그냥 다 지울거에요)
training_data = training_data.dropna(how='any')

# 독립변수들 이상치 처리
zscore_threshold = 1.9

for column in training_data.columns:
    outlier_remove = ~(np.abs(stats.zscore(training_data[column]))>zscore_threshold)
    training_data = training_data.loc[outlier_remove]
```

#### 정규화 (sklearn 할 때는 필요없다)

```python
# 독립변수가 많고, 각각의 데이터 범위(range)가 다를 때는 0~1로 통일 (즉, 정규화 필요!)

# Min Max Normalization
# data 정규화의 가장 일반적인 방법 (사실, StandardScaler도 많이 쓰임)
# 장점 : 모든 feature의 값을 최소값0 ~ 최대값1 로 변환!
# 단점 : 이상치에 민감함 / 이상치를 반드시 처리해야함

# 라이브러리
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# step1. 독립변수용, 종속변수용 MinMaxScaler 객체 생성
scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()

# step2. 각 객체에 scale 할 data 자료를 넣어준다.
scaler_x.fit(training_data[['Temp', 'Wind', 'Solar.R']].values) #==> column 값이 2개 이상이여서 value값 2차원 matrix로 나온다!
scaler_y.fit(training_data['Ozone'].values.reshape(-1,1))

# step3. 넣어줬던 data를 scale 적용 값으로 변화시켜준다.
scaled_data_x = scaler_x.transform(training_data[['Temp', 'Wind', 'Solar.R']].values)
scaled_data_y = scaler_y.transform(training_data['Ozone'].values.reshape(-1,1))
```

#### 모델

```python
# model 생성
model = linear_model.LinearRegression()
# model 학습
model.fit(training_data[['Temp', 'Wind', 'Solar.R']].values,
          training_data['Ozone'].values)
# 가중치랑 bias값 뭐지
print('W: {}, b: {}'.format(model.coef_, model.intercept_))
# 예측
sklearn_prediction = model.predict([[80, 10, 150]])
print(sklearn_prediction) # ==> [[38.8035437]]
```

---



## Logistic

> - 미지의 data에 대해 결과가 어떤 종류의 값으로 `분류`될 수 있는지 예측하는 작업
> - ex) Email-spam 판별 / 주가 오를지 떠러질지 / MRL 사진으로 악성 종양 판별 / 신용카드 사용시 도난 여부
> - `predict 모델 기준`으로 영역을 구분하여 판별하는 것!
> - `Classification` 알고리즘 중 정확도가 상당히 높은 알고리즘
> - 그래서 `Deep Learning 기본 요소(component)`로 사용!
> - linear regression은 기본적으로 `직선`
> - data 범위에 따라 결과가 0 과 1 사이를 넘어서 나올 수 있다.
> - 즉, training data set 에 따라 정확하지 않은 model이 나올 수 있다.
> - 이 문제를 해결하기 위해서 **`logistic regression`** 이 필요하다.
> - 즉, 직선이 아니라 `S자 모양`의 곡선

```python
# 라이브러리
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

# 데이터 로드
df = pd.read_csv('./data/bmi.csv',skiprows=3)

# 데이터 탐색

df.info()
df.describe()

# 결측치 확인
df.isna().sum()

# 이상치 확인
fig = plt.figure()
height_fig = fig.add_subplot(1,2,1)
weight_fig = fig.add_subplot(1,2,2)
height_fig.boxplot(df['height'])
weight_fig.boxplot(df['weight'])

fig.tight_layout()
plt.show()



```

