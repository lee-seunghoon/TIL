# 라이브러리

```python
# 데이터 전처리 위한 넘파이, 판다스
import numpy as np
import pandas as pd

# 그래프 시각화 위한 라이브러리
import matplotlib.pyplot as plt
import seaborn as sns

# 데이터 분리, CV score, KFold 분리
from sklearn.model_selection import train_test_split, cross_val_score, KFold

# 회귀
# 선형 회귀
from sklearn.linear_model import LinearRegression
# KNN 회귀
from sklearn.neighbors import KNeighborsRegressor
# 의사결정나무
from sklearn.tree import DecisionTreeRegressor
# 랜덤포레스트
from sklearn.ensemble import RandomForestRegressor
# 그레디언트 부스팅
from sklearn.ensemble import GradientBoostingRegressor

# 분류
# 로지스틱
from sklearn.linear_model import LogisticRegression
# LinearSVM
from sklearn.svm import LinearSVC
# knn
from sklearn.neighbors import KNeighborsClassifier
# 의사결정나무
from sklearn.tree import DecisionTreeClassifier
# 랜덤포레스트
from sklearn.ensemble import RandomForestClassifier
# 그레디언트 부스팅
from sklearn.ensemble import GradientBoostingClassifier

# 통계값 라이브러리
import statsmodels.api as sm
# VIF 구하는 라이브러리
from statsmodels.stats.outliers_influence import variance_inflation_factor

# confusion matrix
# classification report
from sklearn.metrics import confusion_matrix, classification_report

# 에러메세지 처리
import warnings
warnings.filterwarnings(action='ignore')

# 파라미터 튜닝
# 그리드 서치 & 랜덤 서치
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# 정규화
from sklearn.preprocessing import MinMaxScaler, StandardScaler
```



# 머신러닝 모델 Class

```python
# LSV 측정할 수 있는 클래스 생성
class lsv_ml():

    # 초기값 매개변수로 기본 데이터 부여
    def __init__(self, data):
        self.df = data


    # 데이터 정의
    def data_split(self, var_name):

        # 사용할 데이터 추출
        self.use_df = self.df[self.df[var_name].notnull()]    
        
        # 원인변수(독립변수) 정의
        self.x_data = self.use_df.iloc[:,26:]
        
        # 결과변수(종속변수) 정의
        # 데이터 타입이 문자열일 경우
        if self.use_df[var_name].dtype == 'O':
            mapping = {'위험':0, '하위':1, '상위':2, '우수':3}
            self.y_data = self.use_df[var_name].map(mapping)
        # 데이터 타입이 문자열이 아니라 숫자일 경우
        else:
            self.y_data = self.use_df[var_name]

        return self.x_data, self.y_data

    
    # 데이터 개수 확인
    def eda(self):
        print('현재 모델 데이터 총 개수 :', len(self.y_data))
        print('-'*40) 
        print('각 class 개수\n', self.y_data.value_counts())


    # 데이터 결측치 제거
    def empty_preprocessing(self, x_data):

        # 5-1.입사채널 결측값(5)  
        # 5-2.입사유형 결측값(6)  
        x_data.iloc[:,5] = x_data.iloc[:,5].fillna(x_data.iloc[:,5].value_counts().index[0])
        x_data.iloc[:,6] = x_data.iloc[:,6].fillna(x_data.iloc[:,6].value_counts().index[0])

        # 8-2.최종학력 결측값(11) 
        x_data.iloc[:,11] = x_data.iloc[:,11].fillna(x_data.iloc[:,11].value_counts().index[0])

        # 9-1.거주지 결측값(13)  
        x_data.iloc[:,13] = x_data.iloc[:,13].fillna(x_data.iloc[:,13].value_counts().index[0])

        # 9-2.근무지거리 결측값(14) 
        distance_mean = round(x_data.iloc[:,14].mean(),2)
        x_data.iloc[:,14] = x_data.iloc[:,14].fillna(distance_mean)

        # 모두 0으로 변경할 데이터
        x_data.iloc[:,15:25] = x_data.iloc[:,15:25].fillna(0)
        x_data.iloc[:,26:31] = x_data.iloc[:,26:31].fillna(0)
        x_data.iloc[:,34:45] = x_data.iloc[:,34:45].fillna(0)
        x_data.iloc[:,47:61] = x_data.iloc[:,47:61].fillna(0)
        x_data.iloc[:,96:99] = x_data.iloc[:,96:99].fillna(0)
        x_data.iloc[:,100:111] = x_data.iloc[:,100:111].fillna(0)
        x_data.iloc[:,112:128] = x_data.iloc[:,112:128].fillna(0)
        x_data.iloc[:,130:136] = x_data.iloc[:,130:136].fillna(0)

        # 7-2.결혼기간
        x_data.iloc[:,9] = x_data.iloc[:,9].fillna(x_data.iloc[:,9].value_counts().index[0])

        # 14.출신 대학교 결측값(25)
        x_data.iloc[:,25] = x_data.iloc[:,25].fillna(int(x_data.iloc[:,25].value_counts().index[x_data.iloc[:,25].value_counts().argmax()]))

        # 32.조직 경험 횟수 ~ 33.직무 경험 횟수
        x_data.iloc[:,69] = x_data.iloc[:,69].fillna(int(x_data.iloc[:,69].value_counts().index[x_data.iloc[:,69].value_counts().argmax()]))
        x_data.iloc[:,70] = x_data.iloc[:,70].fillna(int(x_data.iloc[:,70].value_counts().index[x_data.iloc[:,69].value_counts().argmax()]))

        # 34-2.Email_inDegCen내향연결중심성 ~ 34-24.Q3_Clo근접중심성
        for column in x_data.columns[71:87]:
            median = x_data[column].median()
            x_data[column] = x_data[column].fillna(median)

        # 40.중심조직별 연령 비율
        x_data.iloc[:,92] = x_data.iloc[:,92].fillna(x_data.iloc[:,92].median())

        # 42.일 평균 근무시간 결측값(95)
        x_data.iloc[:,95] = x_data.iloc[:,95].fillna(8)

        # 44-1.해당년도 근무일수 결측값(99) 
        x_data.iloc[:,99] = x_data.iloc[:,99].fillna(x_data.iloc[:,99].median())

        # 64.사내커플여부 결측값(129)
        x_data['64.사내커플여부'][(x_data['7-1.결혼상태'] == 2) | (x_data['7-1.결혼상태'] == 3)] = \
        x_data['64.사내커플여부'][(x_data['7-1.결혼상태'] == 2) | (x_data['7-1.결혼상태'] == 3)].fillna(2)
        x_data['64.사내커플여부'][(x_data['7-1.결혼상태'] == 1)] = \
        x_data['64.사내커플여부'][(x_data['7-1.결혼상태'] == 1)].fillna(0)

        # 65-1.과거 2년 E 평가 받은 횟수 결측값(130)
        x_data.iloc[:,129] = x_data.iloc[:,129].fillna(2)

        # 22-2.부서에 따른 출장비용 비율 결측값(46)
        mid = x_data.iloc[:,46][x_data.iloc[:,46]<=100].median()
        x_data.iloc[:,46][x_data.iloc[:,46]>100] = mid
        x_data.iloc[:,46] = x_data.iloc[:,46].fillna(x_data.iloc[:,46].median())

        # 공백 데이터 처리
        if len(x_data['5-1.입사채널'][x_data['5-1.입사채널']==' ']) > 0:
            x_data['5-1.입사채널'][x_data['5-1.입사채널']==' '] = 1
            x_data['5-1.입사채널'] = x_data['5-1.입사채널'].astype(np.int64)

        if len(x_data['9-1.거주지'][x_data['9-1.거주지']==' ']) > 0:
            x_data['9-1.거주지'][x_data['9-1.거주지']==' '] = 13
            x_data['9-1.거주지'] = x_data['9-1.거주지'].astype(np.int64)

        return x_data


    # vif 확인 및 적용 변수
    def vif(self, x_data):

        # VIF를 표현한 데이터프레임 생성
        vif_df = pd.DataFrame()

        # 컬럼명으로 독립변수명을 설정
        vif_df['features'] = x_data.columns

        # statsmodels 라이브러리에서 variance_influence_factor 모듈을 활용한다.
        # 독립변수들 간의 관계를 확인하는 게 다중공선성이라서 독립변수만 입력한다.
        vif_df['VIF'] = [variance_inflation_factor(x_data.values, i) for i in range(x_data.shape[1])]

        # vif 10 초과 변수 제거
        delete_var = vif_df['features'][vif_df['VIF']>10]
        vx_data = x_data.drop(delete_var, axis=1, inplace=False)

        return vx_data
    
    # 모델 학습
    def clf_modeling(self, x_data, y_data, test_ratio, method):
        
        # 모델 생성

        # 모델이 분류일 경우(method == 1)
        if method == 1:

            # 학습, 검정 데이터 분리
            self.train_x, self.test_x, self.train_y, self.test_y = \
            train_test_split(x_data, y_data, test_size=test_ratio, stratify=y_data, random_state=0)

            model = GradientBoostingClassifier(random_state=3).fit(self.train_x, self.train_y)
            
            # 튜닝 조건
            param_grid = {
                'criterion':['friedman_mse','mse','mae'],
                'n_estimators': range(81,111),
                'max_features' : range(1,31),
                'max_depth': range(1,31),
                'min_samples_leaf': range(1,31),
                'learning_rate': np.arange(0.01,0.5,0.05)
            }
            # 랜덤서치 튜닝
            random = RandomizedSearchCV(model, 
                            param_grid, 
                            scoring='accuracy', 
                            return_train_score=True,
                            n_jobs=-1, 
                            verbose=2)
            random.fit(self.train_x, self.train_y)
            self.new_model = GradientBoostingClassifier(random_state=3).set_params(**random.best_params_).fit(self.train_x, self.train_y)
        
        # 모델이 회귀일 경우(method == 2)
        elif method == 2:
            
            # 학습, 검정 데이터 분리
            self.train_x, self.test_x, self.train_y, self.test_y = \
            train_test_split(x_data, y_data, test_size=test_ratio, random_state=0)

            self.new_model = GradientBoostingRegressor(random_state=3).fit(self.train_x, self.train_y)

        return self.new_model


    # 결과 그래프
    def result_graph(self,x, y):

        # 모델이 분류일 경우 confusion matrix와 변수 중요도 모두 확인한다
        if str(self.new_model).find('Regressor') == -1 :
            # classification report
            pred = self.new_model.predict(x)
            print(classification_report(y, pred))
            
            plt.rcParams['font.family'] = 'Malgun Gothic'

            plt.figure()
            plt.show()

            # 매트릭스 히트맵
            matrix = confusion_matrix(y, pred)
            fig, ax = plt.subplots()
            sns.heatmap(pd.DataFrame(matrix),annot=True,cmap='YlGnBu',fmt='g',square=True,annot_kws={'size':20})
            ax.xaxis.set_label_position('top')
            plt.ylabel('Actual Label')
            plt.xlabel('Predict Label')
            plt.show()

            # 변수 중요도
            ft_importance = pd.Series(self.new_model.feature_importances_, index=x.columns).sort_values(ascending=False)
            plt.figure(figsize=(12,15))
            plt.title('변수 특성 중요도')
            sns.barplot(ft_importance, ft_importance.index)
            plt.xlabel('중요도')
            plt.ylabel('원인변수')
            plt.tight_layout()
            plt.show()
        
        # 회귀는 매트릭스와 classification report를 출력할 수 없다.
        # 그래서 정확도 개념과 변수 중요도만 출력 가능하다.
        elif str(self.new_model).find('Regressor') > -1:
            
            # 정확도 (R2 결정계수)
            print('Train Accuracy :', self.new_model.score(self.train_x, self.train_y))
            print('Test Accuracy :', self.new_model.score(self.test_x, self.test_y))
            print()

            # 변수 중요도
            ft_importance = pd.Series(self.new_model.feature_importances_, index=x.columns).sort_values(ascending=False)
            plt.figure(figsize=(12,15))
            plt.title('변수 특성 중요도')
            sns.barplot(ft_importance, ft_importance.index)
            plt.xlabel('중요도')
            plt.ylabel('원인변수')
            plt.tight_layout()
            plt.show()
```

