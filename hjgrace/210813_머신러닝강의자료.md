## Linear Regression Model (선형 회귀 모델)

> - 어떠한 `데이터`에 대해서 그 값에 영향을 주는 `조건(회귀계수)`을 고려하여 데이터의 `평균`을 구하기 위한 함수
> - 평균이라는 표현은 어떤 의미?? >>> 평균적으로' 라는 표현은 `대표성`을 나타낸다. (But, 극단의 이상치가 존재하면 대표성이 깨진다.)
> - 그래서 Regression Model(회귀모델) 다시 정의 ==> 어떠한 `데이터`에 대해서 그 값에 영향을 주는 `조건(회귀계수)`을 고려하여 그 데이터를 `가장 잘 표현하는 함수`
> - 독립변수가 1개인 함수를 가정 ==> `Υ = β0 + β¹x` ==> Y = ax + b (직선!) ==> `β0` : 기타 영향을 주는 요인 / `β¹` : x에 영향을 주는 요인
> - 결론 : Regression Model은 주어진 데이터를 가장 잘 표현하는 `직선`을 찾는 것으로 정의할 수 있다.



### 진짜 쉽게 식으로 표현

> - y = Wx + b 
> - `W`:학습 가중치
> - `b`:bias
> - 초기 W와 b의 값은 랜덤 값



### `sklearn` / simple linear regression 구현

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model

#1. training data set
x_data = np.array([1,2,3,4,5]).reshape(5,1)
t_data = np.array([3,5,7,9,11]).reshape(5,1)

# 산점도 그려보기
plt.scatter(x_data, t_data)
plt.xticks(np.arange(1,6,1))
plt.show()

#2. 모델 만들어보기
model = linear_model.LinearRegression()

#3. 학습
# model.fit(입력값, 레이블값)
# model.fit(독립변수, 종속변수)
model.fit(x_data,t_data)

#4. Weight와 bias 확인해보자
print('W:',model.coef_, 'b:', model.intercept_)

#5. predict
print(model.predict([[9]]))
```



### Ozone data 실습

> - 단순 선형 회귀 모델

```python
import numpy as np
import pandas as pd
from sklearn import linear_model
import matplotlib.pyplot as plt
from scipy import stats # ==> 이상치 제거

##################################################################
# 데이터 탐색
df = pd.read_csv('/content/drive/MyDrive/Google Colab/ozone.csv')

# 독립변수 : Ozone / 종속변수 : Temp
new_df = df[['Ozone','Temp']]

# null 값 확인
new_df.info()

# 전체 통계수치 간단히 확인
new_df.describe()

# 전체 Nan 개수 확인
new_df.isna().sum()

##################################################################

# 데이터 전처리
# Nan 삭제
new_df = new_df.dropna(axis=0, how='any')
new_df

# 결측치 채우기
new_df['Ozone'] = new_df['Ozone'].fillna(int(new_df['Ozone'].mean()))

# boxplot
# ozone
plt.boxplot(new_df['Ozone'],showmeans=True)
plt.show()
# temp
plt.boxplot(new_df['Temp'],showmeans=True)
plt.show()

# 이상치 제거
# zscore 임계치
zscore_threshold = 1.9
outlier_remove = ~(np.abs(stats.zscore(new_df['Ozone']))>zscore_threshold)
new_df = new_df[outlier_remove]

##################################################################
# 독립변수 데이터셋, 종속변수 데이터셋 구축
x_data = new_df['Temp'].values.reshape(-1,1)
t_data = new_df['Ozone'].values.reshape(-1,1)

##################################################################
# 모델 생성
model = linear_model.LinearRegression() # ==> 선형회귀 모델 생성

##################################################################
# 학습(fit)
model.fit(x_data, t_data)

##################################################################
# W와 b 값 확인
print('W:', model.coef_, 'b:', model.intercept_)
# ==> W: [[2.4287033]] b: [-146.99549097]

##################################################################
# predict
prediction = model.predict([[85]])
print(prediction)

##################################################################
# 그래프 그리기
plt.scatter(x_data, t_data)
plt.plot(x_data, (model.coef_ * x_data) + model.intercept_, color='red')
plt.show()

```



> - 다중 선형 회귀 모델

```python
import numpy as np
import pandas as pd
from scipy import stats # ==> zscore 이상치 제거시 사용
from sklearn import linear_model

from sklearn.preprocessing import MinMaxScaler, StandardScaler
# ==> Min Max Normalization 정규화 작업 시 사용


# 데이터 불러오기
df = pd.read_csv('/content/drive/MyDrive/Google Colab/ozone.csv')
training_data = df[['Temp', 'Wind', 'Solar.R', 'Ozone']]

# 데이터 탐색
training_data.info()
training_data.describe()

# 결측치 개수 확인
training_data.isna().sum()

# 결측치 처리 (그냥 다 지울거에요)
training_data = training_data.dropna(how='any')

# 독립변수들 이상치 처리
zscore_threshold = 1.9

for column in training_data.columns:
    outlier_remove = ~(np.abs(stats.zscore(training_data[column]))>zscore_threshold)
    training_data = training_data.loc[outlier_remove]
```

#### 정규화 (sklearn 할 때는 필요없다)

```python
# 독립변수가 많고, 각각의 데이터 범위(range)가 다를 때는 0~1로 통일 (즉, 정규화 필요!)

# Min Max Normalization
# data 정규화의 가장 일반적인 방법 (사실, StandardScaler도 많이 쓰임)
# 장점 : 모든 feature의 값을 최소값0 ~ 최대값1 로 변환!
# 단점 : 이상치에 민감함 / 이상치를 반드시 처리해야함

# 라이브러리
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# step1. 독립변수용, 종속변수용 MinMaxScaler 객체 생성
scaler_x = MinMaxScaler()
scaler_y = MinMaxScaler()

# step2. 각 객체에 scale 할 data 자료를 넣어준다.
scaler_x.fit(training_data[['Temp', 'Wind', 'Solar.R']].values) #==> column 값이 2개 이상이여서 value값 2차원 matrix로 나온다!
scaler_y.fit(training_data['Ozone'].values.reshape(-1,1))

# step3. 넣어줬던 data를 scale 적용 값으로 변화시켜준다.
scaled_data_x = scaler_x.transform(training_data[['Temp', 'Wind', 'Solar.R']].values)
scaled_data_y = scaler_y.transform(training_data['Ozone'].values.reshape(-1,1))
```

#### 모델

```python
# model 생성
model = linear_model.LinearRegression()
# model 학습
model.fit(training_data[['Temp', 'Wind', 'Solar.R']].values,
          training_data['Ozone'].values)
# 가중치랑 bias값 뭐지
print('W: {}, b: {}'.format(model.coef_, model.intercept_))
# 예측
sklearn_prediction = model.predict([[80, 10, 150]])
print(sklearn_prediction) # ==> [[38.8035437]]
```

---



## Logistic

> - 미지의 data에 대해 결과가 어떤 종류의 값으로 `분류`될 수 있는지 예측하는 작업
> - ex) Email-spam 판별 / 주가 오를지 떠러질지 / MRL 사진으로 악성 종양 판별 / 신용카드 사용시 도난 여부
> - `predict 모델 기준`으로 영역을 구분하여 판별하는 것!
> - `Classification` 알고리즘 중 정확도가 상당히 높은 알고리즘
> - 그래서 `Deep Learning 기본 요소(component)`로 사용!
> - linear regression은 기본적으로 `직선`
> - data 범위에 따라 결과가 0 과 1 사이를 넘어서 나올 수 있다.
> - 즉, training data set 에 따라 정확하지 않은 model이 나올 수 있다.
> - 이 문제를 해결하기 위해서 **`logistic regression`** 이 필요하다.
> - 즉, 직선이 아니라 `S자 모양`의 곡선

```python
# 라이브러리
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# bmi 데이터
# 다중 분류 (Multinomial Classification)
# label 0: thin
# label 1: normal
# label 2: fat

# 데이터 로드
df = pd.read_csv('./data/bmi.csv',skiprows=3)

# 데이터 탐색

df.info()
df.describe()

# 결측치 확인
df.isna().sum()

# 이상치 확인
fig = plt.figure()
height_fig = fig.add_subplot(1,2,1)
weight_fig = fig.add_subplot(1,2,2)
height_fig.boxplot(df['height'])
weight_fig.boxplot(df['weight'])
fig.tight_layout()
plt.show()

# 독립변수, 종속변수 분리
x_data = df.iloc[:, 1:]
y_data = df['label']

# Train data / Test data 분리
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, stratify=y_data, test_size=0.2, random_state=99)

# 모델 생성, 학습, score
model = LogisticRegression(random_state=1)
model.fit(x_train, y_train)
train_score = model.score(x_train, y_train)
test_score = model.score(x_test, y_test)
print('Train Data Score : {:.3f} \nTest Data Score : {:.3f}'.format(train_score, test_score))
```



## KNN

> - `K 최근접 이웃` 알고리즘
> - `KNN` : 예측할 데이터의 가장 가까운 이웃을 k개 포함하여 동심원을 그린 후 그 안에 들어 있는 값을 활용해서 예측값 출력
> - 즉, 새로운 데이터에서 기존 데이터와 거리를 계산 한 후 이웃을 뽑아 원을 그려 한 그룹으로 묶고, 예측을 수행
> - 회귀에 사용할 때 --> 최근접 있는 값들의 평균을 구한다
> - 분류에 사용할 때 --> 최근접 있는 이웃의 클래스를 반영한다



#### `KNN hyperparameter`

> - 1. 이웃의 수 (k)
>      - K가 작을 경우 : Overfitting (학습 데이터에 직찹! == 지역적인 특성 너무 많이 반영)
>      - K가 클 경우 : Underfitting (전체 평균을 구하는 느낌으로 학습 안한 것과 비슷해진다.)
>   2. 거리 측정 방식
>      - `Euclidean distance` : 두 data 간의 최단 직선거리
>      - `Manhattan distance` : 좌표축 방향으로만 이동
> - KNN을 사용할 때는 독립변수를 반드시 정규화 해줘야 한다.
> - KNN 장점 : 데이터가 많으면 상당히 정확한 결과 도출
> - KNN 단점 : 데이터가 많아서 모든 이웃간의 거리를 계산할 때 시간이 오래걸린다.



#### KNN 오존 회귀

```python
from sklearn.neighbors import KNeighborsRegressor

# 데이터 로드
df = pd.read_csv('/content/drive/MyDrive/Google Colab/ozone.csv')
ozone_df = df[['Temp', 'Wind', 'Solar.R', 'Ozone']]

# 데이터 탐색
ozone_df.info()
ozone_df.describ()

# 결측치 개수
ozone_df.isna().sum()

# 결측치 제거
ozone_df = ozone_df.dropna(how='any')

# 이상치 확인
fig = plt.figure()
temp_fig = fig.add_subplot(1,4,1)
wind_fig = fig.add_subplot(1,4,2)
solar_fig = fig.add_subplot(1,4,2)
ozone_fig = fig.add_subplot(1,4,2)

temp_fig.boxplot(ozone_df['Temp'])
wind_fig.boxplot(ozone_df['Wind'])
solar_fig.boxplot(ozone_df['Solar.R'])
ozone_fig.boxplot(ozone_df['Ozone'])

fig.tight_layout()
plt.show()

# 이상치 제거
zscore_threshold = 1.9
for column in ['Wind', 'Ozone']:
    outlier_remove = ~(np.abs(stats.zscore(ozone_df[column]))>zscore_threshold)
    ozone_df = ozone_df[outlier_remove]

# 독립변수 & 종속변수
x_data = ozone_df[['Temp', 'Wind', 'Solar.R']].values
y_data = ozone_df['Ozone'].values

# 모델 학습 및 스코어
knn_reg_model = KNeighborsRegressor(n_neighbors=1)
knn_reg_model.fit(x_data, y_data)
train_score = knn_reg_model.score(x_data, y_data)
```



#### BMI 분류

```python
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

# 데이터 로드
df = pd.read_csv('./data/bmi.csv',skiprows=3)

# 데이터 탐색

df.info()
df.describe()

# 결측치 확인
df.isna().sum()

# 이상치 확인
fig = plt.figure()
height_fig = fig.add_subplot(1,2,1)
weight_fig = fig.add_subplot(1,2,2)
height_fig.boxplot(df['height'])
weight_fig.boxplot(df['weight'])
fig.tight_layout()
plt.show()

# 독립변수, 종속변수 분리
x_data = df.iloc[:, 1:]
y_data = df['label']

# Train data / Test data 분리
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, stratify=y_data, test_size=0.2, random_state=99)

# 모델
knn_model = KNeighborsClassifier(n_neighbors = 1)
knn_model.fit(x_train, y_train)
train_score = knn_model.score(x_train, y_train)
test_score = knn_model.score(x_test, y_test)
print('Train Data Score : {:.3f} \nTest Data Score : {:.3f}'.format(train_score, test_score))
```

