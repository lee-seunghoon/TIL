# 라이브러리

```python
# 정규표현식을 위한 라이브러리
import re
# 넘파이 & 판다스
import numpy as np
import pandas as pd
# 그래프 시각화 라이브러리
import matplotlib.pyplot as plt
# 형태소 분석기 라이브러리
from konlpy.tag import Okt
# 빈도분석, tfidf 라이브러리
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
# 코사인 유사도 라이브러리
from sklearn.metrics.pairwise import cosine_similarity
# 에러 메세지 처리 라이브러리
import warnings
warnings.filterwarnings(action='ignore')

# matplotlib 그래프 속성 설정
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['figure.figsize'] = (12,16)
plt.rcParams['font.size'] = 20
```



# 사용자 정의 함수

```python
# 텍스트 전처리 함수
def prepro_text(data, stop_words):
    # okt 형태소 분석기 객체 생성
    okt = Okt()
    # 텍스트 데이터 문자열로 통일
    data = str(data)
    
    # 영문, 특수기호, 숫자 제거
    text = re.sub(r'[^가-힣]', ' ', data)

    prepro_words = []
    # 품사 처리
    for word, tag in okt.pos(text):
        # josa와 suffix 품사 제외한 단어만 추가
        if tag not in ['Josa', 'Suffix']:
            prepro_words.append(word)
    
    # 한 문장으로 묶기
    result = ' '.join(prepro_words)

    return prepro_words


# 빈도분석 후 결과 출력
def count_analyze(texts, count_vec, cnt, color, title):
    
    # 빈도분석용 vectorizer 적합 시키기
    count_vec.fit(texts)
    
    # 단어 사전 정렬
    word_dict = sorted(count_vec.vocabulary_.items())
    # {index : word} 형식으로 단어사전 만들기
    idx2word = {idx:word for word, idx in word_dict}

    # 전체 텍스트를 하나로 묶기
    total_text = []
    total_text.append(' '.join(texts.values))

    # 앞에서 적합(fit) 시킨 vectorizer를 활용해서 전체 텍스트 백터화 하기
    count_matrix = count_vec.transform(total_text)

    count_word = []
    count_vector = []
    for i in range(cnt,0,-1):
        # {index:word} 사전을 활용해서 제일 빈도높은 순서대로 단어 추출하기
        count_word.append(idx2word[(-count_matrix.toarray()[0]).argsort()[i-1]])
        # 빈도높은 횟수를 순서대로 추출하기
        count_vector.append(count_matrix.toarray()[0][(-count_matrix.toarray()[0]).argsort()[i-1]])

    # 추출한 단어와 빈도 확인하기
    print(count_word)
    print(count_vector)

    # 가로 막대 그래프로 보여주기
    plt.barh(count_word, count_vector, color=color)
    plt.yticks(count_word)
    plt.title(f'{title} 빈도 분석')
    plt.show()

    return count_word, count_vector


# 코사인 유사도 score 추출 함수
def cosine_extraction(vec, fit_data, analysis_data, count_words):

    # label1인 텍스트 데이터로 적합
    vec.fit(fit_data)

    # 빈도분석으로 뽑은 Top10 단어들 하나의 text로
    top10_word_vec = vec.transform([' '.join(count_words)])     

    # 분석할 데이터 백터화 하기
    analysis_vec = vec.transform(analysis_data['전체 자소서'])
    
    # Top10 단어들과 분석할 데이터 매칭하여 코사인 유사도 구하기
    cosine_sim = cosine_similarity(top10_word_vec, analysis_vec)

    # 코사인 유사도 데이터 프레임 구성하기
    fit_cosine_df = pd.DataFrame({
        '성명' : analysis_data['성명'].values,
        f'{fit_data.name} 코사인 유사도' : cosine_sim[0]
    })

    # 코사인 유사도를 기준으로 내림차순 정렬하기
    fit_cosine_df.sort_values(by=f'{fit_data.name} 코사인 유사도', ascending=False, inplace=True)

    # 코사인 유사도를 기준으로 랭킹 부여하기
    fit_cosine_df['rank'] = fit_cosine_df[f'{fit_data.name} 코사인 유사도'].rank(ascending=False).astype(np.int64) 

    return fit_cosine_df
```



# 데이터 로드 및 전처리

```python
# 자소서 데이터 불러오기
path = './test.csv'
df = pd.read_csv(path, encoding='utf8')

# 텍스트 데이터가 50개 이상인 기업 리스트
analysis_list = df['organizationName'].value_counts()[df['organizationName'].value_counts() >= 50]
analysis_list

# 삼성전자만 추출해서 시도
for name in analysis_list.index[:1] :
    # 위 리스트에 해당하는 기업의 자소서 텍스트만 분리
    prepro_text_for_count = df['content'][df['organizationName'] == name]
    # 각 텍스트 전처리 적용
    prepro_text_for_count = prepro_text_for_count.map(prepro_text)
    # 시리즈 name을 기업 이름으로 지정
    prepro_text_for_count.name = name
    
    
# 비교 기업 자소서 데이터 불러오기
h_path = './비교기업_data.xlsx'
# 자소서 raw text
h_df = pd.read_excel(h_path)
# 실제 사용할 데이터 분리 (성명, 자소서질문1, 자소서질문2, ...)
use_df = h_df[['성명'] + list(h_df.columns[20:-1].values)]
# 텍스트 데이터 전처리
for col in use_df.columns[1:]:
    use_df[col] = use_df[col].map(prepro_text)
# 전처리된 자소서 하나로 합치기
use_df['전체 자소서'] = use_df[list(use_df.columns[1:])].apply(lambda x : ''.join(x.values), axis=1)
```



# 빈도 분석 후 코사인 유사도 추출

```python
# 삼성전자로 불용어 사전
stop_words_1word = ['최근', '사회', '이슈', '중요하다고', '생각', '회사', '입사', '견해', '기술', '바랍니다', '시기', '선택', '하십시오', '영향', '끼친', '시간', '인물', '간략히', '본인', '성장', '과정', '가장', '한가지', '현재', '포함', '하시기', '바랍니다', '가상', '작품', '취업', '이유', '있다고', '이루고', '삼성', '전자', '자신', '사건', '위해', '하기', '되는', '하게', '되었습니다', '때문', '입니다', '하여', '지원', '한다고', '합니다', '하지', '않고', '이내', '싶은', '이를', '있을', '입니다', '되고', '싶습니다', '이기', '때문', '있습니다', '이러한', '많은', '사람', '하지만', '모습', '기업', '또한', '구체', '서술', '삼', '성', '전자', '지원', '이유', '후', '입사', '회사', '이루고', '싶은', '꿈', '자', '이내', '기술', '하십시오','가', '하는', '이', '하기로', '차', '수', '있었던', '생각', '입니다', '합니다', '하여', '있습니다', '했습니다', '있는','삼성', '통해', '위해', '사람', '하는', '했습니다', '있는', '통해', '대한', '있었습니다', '하였습니다', '하며', '가지', '분야', '대해', '그리고', '하면서', '해야', '하는', '했습니다', '아니라', '하고', '되어', '이었습니다', '같은', '관련', '바탕']
count_vec_1word = CountVectorizer(
    stop_words=stop_words_1word, 
    min_df = 100, # 최소 빈도 100회 이상 고려
    max_features = 100, # 최대 특징 100개 제한
    ngram_range = (1,1) # 1word 세팅
)

# 삼성전자 자소서의 Top50 빈도의 1word 단어 추출
_word1, _vec1 = count_analyze(prepro_text_for_count, count_vec_1word, 50, 'skyblue', '자소서')
```

