# 머신러닝 모델 비교



## 데이터

> - 약 80여개 칼럼 존재
> - 데이터 개수는 1,000개 내외 일듯
> - 정규화 안돼 있음



## 라이브러리

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
# 교차검증, K-Fold cross validation 을 위한 라이브러리
from sklearn.model_selection import cross_val_score
# 선형 회귀 모델
from sklearn.linear_model import LinearRegression
# 로지스틱 회귀 모델
from sklearn.linear_model import LogisticRegression
# KNN 분류
from sklearn.neighbors import KNeighborsClassifier
# SVM
from sklearn.svm import LinearSVC
# 의사결정나무 모델
from sklearn.tree import DecisionTreeClassifier
# 랜덤포레스트
from sklearn.ensemble import RandomForestClassifier
# 그레이디언트 부스팅
from sklearn.ensemble import GradientBoostingClassifier
# 통계값 라이브러리
import statsmodels.api as sm
# Cross Validation 교차검증 라이브러리
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
# VIF 계산 라이브러리
from statsmodels.stats.outliers_influence import variance_inflation_factor
# 매트릭스 그리기
from sklearn.metrics import confusion_matrix
# 히트맵 라이브러리
import seaborn as sns
# 에러메세지 제거
import warnings
warnings.filterwarnings(action='ignore')
# 다양한 정확도 구하기
from sklearn.metrics import classification_report
# 파라미터 튜닝 그리드서치
from sklearn.model_selection import GridSearchCV
# 랜덤서치
from sklearn.model_selection import RandomizedSearchCV
```



## matplotlib 폰트 설정

```python
# 폰트 설정
plt.rcParams['font.size'] = 15
plt.rcParams['font.family'] = 'Malgun Gothic'
```





## 데이터 로드

```python
path = './test.xlsx'
lsv18 = pd.read_excel(path, sheet_name=0, index_col=False)
lsv19 = pd.read_excel(path, sheet_name=1, index_col=False)
lsv20 = pd.read_excel(path, sheet_name=2, index_col=False)
```



## 데이터 전처리

```python
# 필요없는 컬럼 삭제
lsv18.drop(['사원번호', '비즈니스_mean', '피플_mean', '종합_mean', '질문항목별_mean'], axis=1, inplace=True)

# 7점 척도로 돼 있어서 5점 척도로 바꾸기
lsv18 = lsv18.iloc[:,:-1].apply(lambda x : x/7*5)

# 행의 전체 평균 구하기
lsv18['mean'] = lsv18.mean(axis=1)

# 각 구한 평균의 평균 값 구하기
lsv18['mean'].mean()

# 평균 보다 크면 1, 작으면 0 labeling 하기
lsv18['label'] = lsv18['mean'] > lsv18['mean'].mean()
lsv18['label'] = lsv18['label'].map(lambda x : int(x))
```



### 데이터 Null값 확인

```python
# Null값 개수
for name in md1_X.columns:
    nan = md1_X[name].isnull().sum()
    print('{} ==> {}개'.format(name, nan))
```





## 데이터 분리

> 1. train_test_split
> 2. Cross Validation



### 1. train_test_split

```python
# 독립변수 = X , 종속변수 = y
X = lsv18.iloc[:,:-2]
y = lsv18['label']

# test_size = 0.2 로 분리
train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)
```



### 2. Cross Validation

```python
# 바로 cross validation 적용
from sklearn.model_selection import cross_val_score

score1 = cross_val_score(forest, X, y)
print(score1.mean())
```

```python
from sklearn.model_selection import KFold

cv = 5 # ==> Fold의 수
kfold = KFold(n_splits=cv, shuffle=True, random_state=0)
result = cross_val_score(forest, X, y, cv=kfold)
print('최종 accuracy :', result.mean())
```





## 데이터 통계값 확인

> - OLS 모델

```python
x2 = sm.add_constant(X)
model = sm.OLS(y, x2)
result = model.fit()
result.summary()
```



> - VIF 확인

```python
md1_vif = pd.DataFrame({'변수명':column, 'VIF':variance_inflation_factor(model.exog, i)}
                   for i, column in enumerate(model.exog_names))
md1_vif
```







## 모델 객체 생성

```python
lr = LinearRegression()
lgr = LogisticRegression()
svm = LinearSVC(random_state=1)
knn = KNeighborsClassifier(n_neighbors=10)
tree = DecisionTreeClassifier(random_state=2)
forest = RandomForestClassifier(n_estimators=50, random_state=3)
```



## 학습

```python
lr.fit(train_X, train_y)
lgr.fit(train_X, train_y)
svm.fit(train_X, train_y)
knn.fit(train_X, train_y)
tree.fit(train_X, train_y)
forest.fit(train_X, train_y)
```



## Score 계산

```python
print('선형회귀 정확도 : {:.2f}'.format(lr.score(test_X, test_y)))
print('로지스틱 정확도 : {:.2f}'.format(lgr.score(test_X, test_y)))
print('SVM 정확도 : {:.2f}'.format(svm.score(test_X, test_y)))
print('KNN 정확도 : {:.2f}'.format(knn.score(test_X, test_y)))
print('의사결정나무 정확도 : {:.2f}'.format(tree.score(test_X, test_y)))
print('랜덤포레스트 정확도 : {:.2f}'.format(forest.score(test_X, test_y)))
```



## Traing set, test set 정확도 그래프 비교

> - KNN

```python
train_acc = []
test_acc = []

for n in range(1,15):
    clf = KNeighborsClassifier(n_jobs=-1, n_neighbors=n)
    clf.fit(train_X, train_y)
    prediction = clf.predict(test_X)
    train_acc.append(clf.score(train_X, train_y))
    test_acc.append((prediction==test_y).mean())
    
# 그래프 그리기
plt.figure(figsize=(12,9))
plt.plot(range(1,15), train_acc, label='Train Set')
plt.plot(range(1,15), test_acc, label='Test Set')
plt.xlabel('n_neighbors')
plt.ylabel('accuracy')
plt.xticks(np.arange(0,16, step=1))
plt.legend()
plt.show()
```



> - 디시전 트리

```python
# 디시젼트리의 max_depth 조절을 통해 트리 깊이를 제한한다 ==> 오버피팅 줄일 수있다.
# ==> 사전 가지치기 방법
# 독립변수가 총 32개

train_acc = []
test_acc = []

for n in range(1,33):
    clf = DecisionTreeClassifier(max_depth=n)
    clf.fit(train_X, train_y)
    prediction = clf.predict(test_X)
    train_acc.append(clf.score(train_X, train_y))
    test_acc.append((prediction==test_y).mean())
    
# 그래프 그리기
plt.figure(figsize=(12,9))
plt.plot(range(1,33), train_acc, label='Train Set')
plt.plot(range(1,33), test_acc, label='Test Set')
plt.xlabel('max_depth')
plt.ylabel('accuracy')
plt.xticks(np.arange(1,33, step=1))
plt.legend()
plt.show()
```





## 히트맵

```python
# confusion matrix 만들기
cnf_metrix = metrics.confusion_matrix(y_test, y_pred)
print(cnf_metrix)

# 히트맵 속성 설정
class_names = ['good_leader', 'non_good_leader']
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)

# 히트맨 생성
sns.heatmap(pd.DataFrame(cnf_metrix), annot=True, cmap='YlGnBu', fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title("Confusion matrix", y=1.1)
plt.ylabel("Actual label")
plt.xlabel("Predict label")
plt.show()
```



## 특성 중요도

```python
# 디시젼트리 & 랜덤포레스트 한해 사용
def plt_feature_importances_lsv(model, title):
    n_features = X.shape[1]
    plt.figure(figsize=(12,9))
    plt.barh(np.arange(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), X.columns.values)
    plt.xlabel('{} 특성 중요도'.format(title))
    plt.ylabel('원인변수')
    plt.ylim(-1, n_features)
    plt.tight_layout()
    plt.show()
```



## 하이퍼파라미터 튜닝



### 랜덤 서치

> - 의사결정나무 모델로 smaple

```python
param_grid = {
    'criterion':['gini','entropy'],
    'max_depth' : np.arange(1,11),
    'max_features' : np.arange(1,21),
    'min_samples_leaf' : np.arange(1,11)
}
random_search =  RandomizedSearchCV(tree,
                                   param_grid,
                                   n_iter=50,
                                   scoring='accuracy',
                                   n_jobs=-1,
                                   return_train_score=True,
                                   verbose=2)
random_search.fit(train_X, train_y)

cv_result_df = pd.DataFrame(random_search.cv_results_)
cv_result_df.sort_values(by=['rank_test_score'], inplace=True)
#cv_result_df[['params', 'mean_test_score', 'rank_test_score']].head(10)
cv_result_df

# 모델 파라미터 세팅하는 법
new_tree = DecisionTreeClassifier()
new_tree.set_params(**random_search.best_params_)
new_tree.get_params()
```

