## `Tensorflow 2.x 버전` 사용한 `Linear Regression` 구현 비교

#### <Ozone Data 예제>

- **`Data 전처리`**

  ```python
  import numpy as np
  import pandas as pd
  from sklearn.preprocessing import MinMaxScaler
  from scipy import stats
  ```

  ```python
  # Raw Data
  df = pd.read_csv('../data/ozone.csv')
  
  # 독립변수, 종속변수 나누기
  x_data = df[['Solar.R','Wind','Temp']]
  t_data = df['Ozone']
  ```

  ```python
  # 1. 독립변수에 대한 결측치를 검출한 후 imputation 진행 (평균화기법=median)
  #    median으로 처리하는 이유는 이상치를 처리하지 않았기 때문!
  
  for col in x_data.columns:
      # 현재 data 에서 nan 제외하고 median 구하는 방법 ==> np.nanmedian()
      col_median = np.nanmedian(x_data[col])
      x_data[col].loc[x_data[col].isnull()] = col_median
      
  ```

  ```python
  # 2. 독립변수에 대한 이상치 검출 후 mean값으로 처리
  # 	 이상치 검출 ==> zscore 방식 사용
  zscore_threshold = 1.8
  
  for col in x_data.columns:
      outlier = x_data[col].loc[np.abs(stats.zscore(x_data[col])) > zscore_threshold]
      
      # 한가지 주의! ==> 각 칼럼의 평균값 구할 때, 이상치(outlier)를 제외한 평균값을 구해줘야 한다.
      col_mean = np.mean(x_data[col][~x_data[col].isin(outlier)].vaelues) # 내방식
      # col_mean = x_data[col][~x_data[col].isin(outlier)].mean() # 내방식 2
      # col_mean = np.mean(x_data.loc[~x_data[col].isin(outlier), col]) # teacher 방식
      
      # outlier 값을 mean값으로 대체 하자
      x_data[col][x_data[col].isin(outlier)] = col_mean
      # x_data.loc[x_data[col].isin(outlier), col] = col_mean # teacher 방식
  ```

  ```python
  # 3. 종속 변수에 대한 이상치를 검출한 후 mean값으로 처리
  # 	 이상치 검출 ==> zscore 방식 사용
  
  # 근데 여기서 문제! 현재 Ozone column의 data에는 nan 값이 있어요. 여기서 zscore 처리해줄 때, nan 때문에 값이 안 변해!
  # 그래서 Ozone column의 data에서 nan 값 처리 해주고 zscore 적용
  # outlier = x_data['Ozone'][np.abs(stats.zscore(x_data['Ozone'])) > zscore_threshold] # <<< 이게 아니다
  
  nan_no_ozone_data = t_data[~t_data.isnull()] # ==> 종속변수에서 nan값을 빼야 zscore 처리가 돼
  outlier = nan_no_ozone_data[np.abs(stats.zscore(nan_no_ozone_data)) > zscore_threshold] # ==> nan이 없는 종속변수에서 outlier 택
  
  # nan이 없는 ozone data에서 outlier(이상치)가 아닌 데이터들의 평균값을 구해!
  ozone_mean = nan_no_ozone_data[~nan_no_ozone_data.isin(outlier)].mean() # ==> 36.79816513761468
  
  # 원본 t_data에 있는 outlier 값들을 ozone_mean으로 바꾸자
  t_data[t_data.isin(outlier)] = ozone_mean 
  ```

  ```python
  # 4. 정규화 (KNN에서 사용하려고 정규화 하는거임. 그냥 sklearn은 정규화 필요없음)
  scaler_x = MinMaxScaler()
  scaler_t = MinMaxScaler()
  
  scaler_x.fit(x_data.values)
  scaler_t.fit(t_data.values.reshape(-1,1))
  
  x_data_norm = scaler_x.transform(x_data.values)
  
  # t_data transform 할때, 2차원 matrix 형태로 넣어야 한다.
  # t_data ravel() 하는 이유는 sklearn 넣을 때, 1차원으로 넣어야 해서
  t_data_norm = scaler_t.transform(t_data.values.reshape(-1,1)).ravel()
  ```

  ```python
  # 5. 종속변수에 대한 결측치를 KNN을 이용하여 Imputation(예측기법) 처리
  from sklearn.neighbors import KNeighborsRegressor
  
  # KNN 학습에 사용될 x_data와 t_data를 추려내야 한다.
  # np.isnan(t_data_norm) ==> nan 값이 있으면 True
  
  
  x_data_train_norm = x_data_norm[~np.isnan(t_data_norm)]
  t_data_train_norm = t_data_norm[~np.isnan(t_data_norm)]
  # Q. 왜 .isnull()을 안썼을까??  ==> t_data_norm 형태를 보면 시리즈야? 데이터프레임이야? 아니지! ==> nd.array잖아!
  # ==> 그래서 np.isnan() 을 쓴거지!!
  
  # KNN 모델 만들고
  knn_regressor = KNeighborsRegressor(n_neighbors=2)
  knn_regressor.fit(x_data_train_norm) # 학습 시키고
  # 예측할 때는 종속변수 값이 nan인 행의 독립변수들을 predict하고
  knn_predict = knn_regressor.predict(x_data_norm[np.isnan(t_data_norm)])
  # 종속변수 값이 nan인 값을 knn_predict 값으로 대체 해주고
  t_data_norm[np.isnan(t_data_norm)] = knn_predict
  
  ###################################################
  # 최종적으로 우리가 사용할 Data
  # 독립변수 : x_data_norm
  # 종속변수 : t_data_norm
  ```



- **`Tensorflow 2.x ver. Linear Regression 구현`**

  ```python
  from tensorflow.keras.models import Sequential
  from tensorflow.keras.layer import Flatten, Dense
  from tensorflow.keras.optimizers import SGD
  
  # model 생성
  keras_model = Sequential() # ==> layer를 순차적으로 사용하겠다는 model box
  
  # input layer 생성
  keras_model.add(Flatten(input_shape=(x_data_norm.shape[1],)))
  # input_shape=() : 우리가 생성한 model 안에 input layer를 만들건데, 이 layer 안에 들어오는 하나의 data shape이 뭔지 알려줘야 해!
  # 즉, 여기서 input_shape=() 값을 주는 건... 저 값으로 shape를 만들어 달라는 게 아니라, 
  # 여기 들어오는 x_data == 독립변수들의 1 row의 값이 들어올 때 shape이 뭔지 알려달라는 거야!
  
  # out layer 생성
  keras_model.add(Dense(1, activation='linear'))
  # Dense의 인자로는 몇개의 뉴런(즉, 몇개의 regression model을 사용할건지)
  # Linear Regression model은 1개만 쓰잖아!! 그래서 ==> 1
  # 만약 Binary Classification 이면 Logistic Regression 1개만 쓰잖아 그래서 ==> 1
  # 만약 Multiple Classification 이면 ==> logistic Regression 개수가 label의 유형 개수만큼 늘어나지 그럼 그 숫자 적으면 되는거야
  
  # loss, optimizer 적용 (compile)
  keras_model.compile(optimizer=SGD(learning_rate=1e-2),
                      loss='mse') # mse ==> 평균제곱오차
  
  # 학습
  keras_model.fit(x_data_norm, 
                  t_data_norm,
                  epochs=5000,
                  verbose=0)
  
  # 예측할 data 정규화
  test_data = scaler_x.transform([[310, 15, 80]])
  
  # 예측
  keras_predict = keras_model.predict(test_data)
  print('keras model 예측값 :', scaler_t.inverse_transform(keras_predict.reshape(-1,1)))
  # ==> keras model 예측값 : [[33.381878]]
  ```



- **`sklearn Linear Regression 구현`**

  ```python
  from sklearn.linear_model import LinearRegression
  
  sk_model = LinearRegression()
  sk_model.fit(x_data_norm, t_data_norm)
  sk_predict = sk_model.predict(test_data)
  print('sklearn의 결과 : {}'.format(scaler_t.inverse_transform(sk_predict.reshape(-1,1))))
  # ==> sklearn의 결과 : [[33.2884048]]
  ```

  

---



## Titanic 예제 (`Binary Logistic`) 구현 비교

```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from scipy import stats
```



- **`Data 전처리`**

  ```python
  # Raw Data Load
  df = pd.read_csv('../kaggle_data/titanic/train.csv')
  
  # Featurn Engineering (필요없는 column 지우고, 합칠거 합치고, 문자 data 숫자로 바꾸고)
  
  # 필요없는 column 삭제
  df = df.drop(['PassengerId', 'Name', 'Ticket', 'Fare', 'Cabin'], axis=1, inplace=False)
  
  # column 합치고 기존 것 삭제
  df['Family'] = df['SibSp'] + df['Parch']
  df.drop(['SibSp','Parch'], axis=1, inplace=True)
  
  # 문자 data ==> 숫자 data로 변경
  sex_dict = {'male':0, 'female':1}
  df['Sex'] = df['Sex'].map(sex_dict)
  
  embark_dict = {'S':0, 'C':1, 'Q':2}
  df['Embarked']=df['Embarked'].map(embark_dict)
  ```

  ```python
  # 결측치 처리
  
  # 'Age' 결측치 ==> median으로 대체
  # np.nanmedian() ==> nan값을 제외한 data에서 median 값을 return 해준다.
  df['Age'][df['Age'].isnull()] = np.nanmedian(df['Age'].values)
  
  # 'Embark' 결측치 ==> 최빈값(mode)으로 대체
  # 최빈값 구하기 : stats.mode(data, nan_policy='nan 어떻게 처리할 지 입력')
  embark_mode = stats.mode(df['Embarked'], nan_policy='omit')[0][0] 
  # 모드 객체로 떨어지는 데 그 중 첫번째가 최빈값이고, nd.array 형태여서 그 안에 값을 가져오기 위해 한번더 [0] 인덱스 
  df['Embarked'][df['Embarked'].isnull()] = embark_mode
  
  # 이상치 처리는 안해요.
  ```

  ```python
  # Age 값을 category 분류
  # 함수와 map을 사용해서 손쉽게 분류!
  
  # age값 범주 나누는 함수 생성
  def category_age(age):
      if (age >= 0) & (age < 20):
          return 0
      elif (age >= 20) & (age < 50):
          return 1
      else:
          return 2
  
  # 시리즈에 map 함수를 사용하고 인자로 함수를 주면, 시리즈 안의 값들을 각각 인자로 준 함수로 처리한다.
  df['Age'] = df['Age'].map(category_age)
  ```

- **`Data train-validation 분류`**

  ```python
  x_data_train, x_data_test, t_data_train, t_data_test = \
  train_test_split(df.drop('Survived', axis=1, inplace=False),
                   df['Survived'],
                   test_size=0.3,
                   random_state=0)
  ```

- **`정규화`**

  ```python
  scaler = MinMaxScaler()
  scaler.fit(x_data_train)
  
  norm_x_data_train = scaler.transform(x_data_train)
  norm_x_data_test = scaler.transform(x_data_test)
  ```



- **`sklearn 구현`**

  ```python
  from sklearn.linear_model import LogisticRegression
  sk_model = LogisticRegression()
  sk_model.fit(norm_x_data_train, t_data_train)
  sk_accuracy = sk_model.score(norm_x_data_test, t_data_test)
  print('sklearn의 accuracy =', sk_accuracy) # ==> sklearn의 accuracy = 0.7947761194029851
  ```



- **`Tensorflow 2.x Keras ver. 구현`**

  ```python
  from tensorflow.keras.models import Sequential
  from tensorflow.keras.layers import Flastten, Dense
  from tensorflow.keras.optimizers import SGD
  
  keras_model = Sequential()
  keras_model.add(Flatten(input_shape=(norm_x_data_train.shape[1],)))
  keras_model.add(Dense(1, activation='sigmoid'))
  keras_model.compile(optimizer=SGD(learning_rate=1e-2),
                      loss='binary_crossentropy', # ==> binary classification loss 만들 때, crossentropy 적용
                      metrics=['accuracy']) # 성능 평가 방법으로는 accuracy로 진행
  keras_model.fit(norm_x_data_train,
                  t_data_train,
                  epochs=5000,
                  verbose=0)
  
  # 평가
  keras_accuracy = keras_model.evaluate(norm_x_data_test, t_data_test)
  print('keras의 accuracy =', keras_accuracy) # ==> keras의 accuracy = [0.44817766547203064, 0.7947761416435242]
  ```

  