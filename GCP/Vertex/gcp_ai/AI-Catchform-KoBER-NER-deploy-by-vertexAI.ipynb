{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8abcc6d3",
   "metadata": {},
   "source": [
    "### 참고 url\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pytorch/pytorch-text-sentiment-classification-custom-train-deploy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8279a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages required for executing this notebook.\n",
    "\n",
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip install --upgrade google-cloud-aiplatform {USER_FLAG} -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c031bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7029d087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boki7996'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UUID\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()\n",
    "UUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72069f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP project id & region 설정\n",
    "\n",
    "PROJECT_ID = 'airy-runway-344101'\n",
    "REGION = \"asia-east1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29490c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://catch-ai/data/\n",
      "                                 gs://catch-ai/model/\n",
      "                                 gs://catch-ai/pytorch-on-gcp/\n"
     ]
    }
   ],
   "source": [
    "# GCS 버킷 세팅\n",
    "BUCKET_NAME = \"catch-ai\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "# 버킷 접속 확인\n",
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0c6c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries for this notebook.\n",
    "\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf.json_format import MessageToDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "902df411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the constants needed for this tutorial.\n",
    "\n",
    "# Name for the package application / model / repository\n",
    "APP_NAME = \"ai-catchform-kobert\"\n",
    "\n",
    "# URI for the pre-built container for custom training\n",
    "PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI = (\n",
    "    \"asia-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-11:latest\"\n",
    ")\n",
    "\n",
    "# Name of the folder where the python package needs to be stored\n",
    "PYTHON_PACKAGE_APPLICATION_DIR = \"ai_catchform\"\n",
    "\n",
    "# Path to the source distribution tar of the python package\n",
    "source_package_file_name = f\"{PYTHON_PACKAGE_APPLICATION_DIR}/dist/trainer-0.1.tar.gz\"\n",
    "\n",
    "# GCS path where the python package is stored\n",
    "python_package_gcs_uri = (\n",
    "    f\"{BUCKET_URI}/pytorch-on-gcp/{APP_NAME}/train/python_package/trainer-0.1.tar.gz\"\n",
    ")\n",
    "\n",
    "# Module name for training application\n",
    "python_module_name = \"src.main\"\n",
    "\n",
    "# Training job's display name\n",
    "JOB_NAME = f\"{APP_NAME}-pytorch-pkg-train-{UUID}\"\n",
    "\n",
    "# Set training job's machine-type\n",
    "TRAIN_MACHINE_TYPE = \"n1-standard-8\"\n",
    "# Set training job's accelerator type\n",
    "TRAIN_ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"\n",
    "# Set no. of h/w accelerators needed for the training job\n",
    "TRAIN_ACCELERATOR_COUNT = 1\n",
    "\n",
    "# Set the name of the container image for prediction\n",
    "CUSTOM_PREDICTOR_IMAGE_URI = (\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{APP_NAME}/pytorch_predict_{APP_NAME}:latest\"\n",
    ")\n",
    "\n",
    "# Set the version for model-deployment\n",
    "VERSION = 1\n",
    "# Set the model display name\n",
    "model_display_name = f\"{APP_NAME}-v{VERSION}\"\n",
    "# Set the model description\n",
    "model_description = \"PyTorch based KoBERT NER classification model for AI-catchform\"\n",
    "\n",
    "# Set the health route for prediction container\n",
    "health_route = \"/ping\"\n",
    "# Set the predict route for prediction container\n",
    "predict_route = f\"/predictions/{APP_NAME}\"\n",
    "# Set the serving container ports for prediction\n",
    "serving_container_ports = [7080]\n",
    "\n",
    "# Set the display name for endpoint\n",
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "# Set the machine-type for deployment\n",
    "DEPLOY_MACHINE_TYPE = \"n1-standard-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "092d5d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Vertex AI SDK for Python\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3684eb56",
   "metadata": {},
   "source": [
    "## Custom Training on Vertex AI\n",
    "\n",
    "### Recommended Training Application Structure\n",
    "\n",
    "You can structure your training application in any way you like. However, the following structure is commonly used in Vertex AI samples, and having your project organized similarly can make it easier for you to follow the samples.\n",
    "\n",
    "The following python_package directory structure shows a sample packaging approach.\n",
    "\n",
    "```\n",
    "├── ai_catchform\n",
    "│   ├── setup.py\n",
    "│   └── trainer\n",
    "│       ├── __init__.py\n",
    "│       ├── experiment.py\n",
    "│       ├── metadata.py\n",
    "│       ├── model.py\n",
    "│       ├── task.py\n",
    "│       └── utils.py\n",
    "└── pytorch-text-sentiment-classification-custom-train-deploy.ipynb    --> This notebook\n",
    "```\n",
    "\n",
    "- Main project directory contains your setup.py file with the dependencies.\n",
    "- Inside trainer directory:\n",
    "    - task.py - Main application module initializes and parse task arguments (hyperparameters). It also serves as an entry point to the trainer.\n",
    "    - model.py - Includes a function to create a model with a sequence classification head from a pre-trained model.\n",
    "    - experiment.py - Runs the model training and evaluation experiment, and exports the final model.\n",
    "    - metadata.py - Defines the metadata for classification tasks such as predefined model, dataset name and target labels.\n",
    "\n",
    "- utils.py - Includes utility functions such as those used for reading data, saving models to Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "521e3909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing requirements to trainer.egg-info/requires.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying setup.py -> trainer-0.1\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/requires.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "# Run the following command to create a source distribution.\n",
    "!cd {PYTHON_PACKAGE_APPLICATION_DIR} && python setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "313af71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://ai_catchform/dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  962.0 B/  962.0 B]                                                \n",
      "Operation completed over 1 objects/962.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "# Now upload the source distribution with the training application to Cloud Storage bucket.\n",
    "!gsutil cp {source_package_file_name} {python_package_gcs_uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89fa2086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       962  2022-12-23T08:09:05Z  gs://catch-ai/pytorch-on-gcp/ai-catchform-kobert/train/python_package/trainer-0.1.tar.gz\n",
      "TOTAL: 1 objects, 962 bytes (962 B)\n"
     ]
    }
   ],
   "source": [
    "# Validate that the source distribution exists in the Cloud Storage bucket.\n",
    "!gsutil ls -l {python_package_gcs_uri}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb1daa55",
   "metadata": {},
   "source": [
    "### Run a custom job in Vertex AI using a pre-built container\n",
    "\n",
    "In this notebook, you are using Hugging Face Datasets and fine-tuning a transformer model from the Hugging Face Transformers library for sentiment analysis tasks using PyTorch. You don't need to build a PyTorch environment from scratch for running the training application because Vertex AI provides pre-built containers.\n",
    "\n",
    "Vertex AI pre-built containers are Docker container images that you can use for custom training. They include some common dependencies used in training code based on the machine learning framework and framework version.\n",
    "\n",
    "You use a pre-built container for PyTorch and the packaged training application to run the training job on Vertex AI.\n",
    "\n",
    "Configure a Custom Job with the pre-built container image for PyTorch and training code packaged as Python source distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c91cd405",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.CustomPythonPackageTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    python_package_gcs_uri=python_package_gcs_uri,\n",
    "    python_module_name=python_module_name,\n",
    "    container_uri=PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25c18e49",
   "metadata": {},
   "source": [
    "### Run the Custom training job with the following parameters:\n",
    "\n",
    "- `machine_type`: Mahcine type on which the job needs to run.\n",
    "- `accelerator_type`: Hardware accelerator type for running the job. One of *ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100, NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4.*\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `replica_count`: The number of worker replicas.\n",
    "- `args`: Command line arguments to be passed to the Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f157ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://catch-ai/aiplatform-custom-training-2022-12-23-17:09:21.576 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1524510392745721856?project=1001227203935\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7495461063941423104?project=1001227203935\n",
      "CustomPythonPackageTrainingJob projects/1001227203935/locations/us-central1/trainingPipelines/1524510392745721856 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/1001227203935/locations/us-central1/trainingPipelines/1524510392745721856 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/1001227203935/locations/us-central1/trainingPipelines/1524510392745721856 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/1001227203935/locations/us-central1/trainingPipelines/1524510392745721856 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/1001227203935/locations/us-central1/trainingPipelines/1524510392745721856 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=1001227203935&resource=ml_job%2Fjob_id%2F7495461063941423104&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%227495461063941423104%22\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m job\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m      2\u001b[0m     replica_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     machine_type\u001b[39m=\u001b[39;49mTRAIN_MACHINE_TYPE,\n\u001b[1;32m      4\u001b[0m     accelerator_type\u001b[39m=\u001b[39;49mTRAIN_ACCELERATOR_TYPE,\n\u001b[1;32m      5\u001b[0m     accelerator_count\u001b[39m=\u001b[39;49mTRAIN_ACCELERATOR_COUNT,\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/works/catchsecu/gcp_ai/gcp-env/lib/python3.8/site-packages/google/cloud/aiplatform/training_jobs.py:6412\u001b[0m, in \u001b[0;36mCustomPythonPackageTrainingJob.run\u001b[0;34m(self, dataset, annotation_schema_uri, model_display_name, model_labels, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, base_output_dir, service_account, network, bigquery_destination, args, environment_variables, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type, reduction_server_container_uri, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, tensorboard, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m   6397\u001b[0m network \u001b[39m=\u001b[39m network \u001b[39mor\u001b[39;00m initializer\u001b[39m.\u001b[39mglobal_config\u001b[39m.\u001b[39mnetwork\n\u001b[1;32m   6399\u001b[0m worker_pool_specs, managed_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_and_validate_run(\n\u001b[1;32m   6400\u001b[0m     model_display_name\u001b[39m=\u001b[39mmodel_display_name,\n\u001b[1;32m   6401\u001b[0m     model_labels\u001b[39m=\u001b[39mmodel_labels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6409\u001b[0m     reduction_server_machine_type\u001b[39m=\u001b[39mreduction_server_machine_type,\n\u001b[1;32m   6410\u001b[0m )\n\u001b[0;32m-> 6412\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\n\u001b[1;32m   6413\u001b[0m     dataset\u001b[39m=\u001b[39;49mdataset,\n\u001b[1;32m   6414\u001b[0m     annotation_schema_uri\u001b[39m=\u001b[39;49mannotation_schema_uri,\n\u001b[1;32m   6415\u001b[0m     worker_pool_specs\u001b[39m=\u001b[39;49mworker_pool_specs,\n\u001b[1;32m   6416\u001b[0m     managed_model\u001b[39m=\u001b[39;49mmanaged_model,\n\u001b[1;32m   6417\u001b[0m     model_id\u001b[39m=\u001b[39;49mmodel_id,\n\u001b[1;32m   6418\u001b[0m     parent_model\u001b[39m=\u001b[39;49mparent_model,\n\u001b[1;32m   6419\u001b[0m     is_default_version\u001b[39m=\u001b[39;49mis_default_version,\n\u001b[1;32m   6420\u001b[0m     model_version_aliases\u001b[39m=\u001b[39;49mmodel_version_aliases,\n\u001b[1;32m   6421\u001b[0m     model_version_description\u001b[39m=\u001b[39;49mmodel_version_description,\n\u001b[1;32m   6422\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   6423\u001b[0m     environment_variables\u001b[39m=\u001b[39;49menvironment_variables,\n\u001b[1;32m   6424\u001b[0m     base_output_dir\u001b[39m=\u001b[39;49mbase_output_dir,\n\u001b[1;32m   6425\u001b[0m     service_account\u001b[39m=\u001b[39;49mservice_account,\n\u001b[1;32m   6426\u001b[0m     network\u001b[39m=\u001b[39;49mnetwork,\n\u001b[1;32m   6427\u001b[0m     training_fraction_split\u001b[39m=\u001b[39;49mtraining_fraction_split,\n\u001b[1;32m   6428\u001b[0m     validation_fraction_split\u001b[39m=\u001b[39;49mvalidation_fraction_split,\n\u001b[1;32m   6429\u001b[0m     test_fraction_split\u001b[39m=\u001b[39;49mtest_fraction_split,\n\u001b[1;32m   6430\u001b[0m     training_filter_split\u001b[39m=\u001b[39;49mtraining_filter_split,\n\u001b[1;32m   6431\u001b[0m     validation_filter_split\u001b[39m=\u001b[39;49mvalidation_filter_split,\n\u001b[1;32m   6432\u001b[0m     test_filter_split\u001b[39m=\u001b[39;49mtest_filter_split,\n\u001b[1;32m   6433\u001b[0m     predefined_split_column_name\u001b[39m=\u001b[39;49mpredefined_split_column_name,\n\u001b[1;32m   6434\u001b[0m     timestamp_split_column_name\u001b[39m=\u001b[39;49mtimestamp_split_column_name,\n\u001b[1;32m   6435\u001b[0m     bigquery_destination\u001b[39m=\u001b[39;49mbigquery_destination,\n\u001b[1;32m   6436\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   6437\u001b[0m     restart_job_on_worker_restart\u001b[39m=\u001b[39;49mrestart_job_on_worker_restart,\n\u001b[1;32m   6438\u001b[0m     enable_web_access\u001b[39m=\u001b[39;49menable_web_access,\n\u001b[1;32m   6439\u001b[0m     tensorboard\u001b[39m=\u001b[39;49mtensorboard,\n\u001b[1;32m   6440\u001b[0m     reduction_server_container_uri\u001b[39m=\u001b[39;49mreduction_server_container_uri\n\u001b[1;32m   6441\u001b[0m     \u001b[39mif\u001b[39;49;00m reduction_server_replica_count \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m\n\u001b[1;32m   6442\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   6443\u001b[0m     sync\u001b[39m=\u001b[39;49msync,\n\u001b[1;32m   6444\u001b[0m     create_request_timeout\u001b[39m=\u001b[39;49mcreate_request_timeout,\n\u001b[1;32m   6445\u001b[0m )\n",
      "File \u001b[0;32m~/works/catchsecu/gcp_ai/gcp-env/lib/python3.8/site-packages/google/cloud/aiplatform/base.py:810\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m    809\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[39m.\u001b[39mwait(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 810\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    812\u001b[0m \u001b[39m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    813\u001b[0m internal_callbacks \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/works/catchsecu/gcp_ai/gcp-env/lib/python3.8/site-packages/google/cloud/aiplatform/training_jobs.py:6697\u001b[0m, in \u001b[0;36mCustomPythonPackageTrainingJob._run\u001b[0;34m(self, dataset, annotation_schema_uri, worker_pool_specs, managed_model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, args, environment_variables, base_output_dir, service_account, network, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, bigquery_destination, timeout, restart_job_on_worker_restart, enable_web_access, tensorboard, reduction_server_container_uri, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m   6678\u001b[0m             spec[\u001b[39m\"\u001b[39m\u001b[39mpython_package_spec\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39menv\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [\n\u001b[1;32m   6679\u001b[0m                 {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: key, \u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: value}\n\u001b[1;32m   6680\u001b[0m                 \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m environment_variables\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   6681\u001b[0m             ]\n\u001b[1;32m   6683\u001b[0m (\n\u001b[1;32m   6684\u001b[0m     training_task_inputs,\n\u001b[1;32m   6685\u001b[0m     base_output_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6694\u001b[0m     tensorboard\u001b[39m=\u001b[39mtensorboard,\n\u001b[1;32m   6695\u001b[0m )\n\u001b[0;32m-> 6697\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_job(\n\u001b[1;32m   6698\u001b[0m     training_task_definition\u001b[39m=\u001b[39;49mschema\u001b[39m.\u001b[39;49mtraining_job\u001b[39m.\u001b[39;49mdefinition\u001b[39m.\u001b[39;49mcustom_task,\n\u001b[1;32m   6699\u001b[0m     training_task_inputs\u001b[39m=\u001b[39;49mtraining_task_inputs,\n\u001b[1;32m   6700\u001b[0m     dataset\u001b[39m=\u001b[39;49mdataset,\n\u001b[1;32m   6701\u001b[0m     annotation_schema_uri\u001b[39m=\u001b[39;49mannotation_schema_uri,\n\u001b[1;32m   6702\u001b[0m     training_fraction_split\u001b[39m=\u001b[39;49mtraining_fraction_split,\n\u001b[1;32m   6703\u001b[0m     validation_fraction_split\u001b[39m=\u001b[39;49mvalidation_fraction_split,\n\u001b[1;32m   6704\u001b[0m     test_fraction_split\u001b[39m=\u001b[39;49mtest_fraction_split,\n\u001b[1;32m   6705\u001b[0m     training_filter_split\u001b[39m=\u001b[39;49mtraining_filter_split,\n\u001b[1;32m   6706\u001b[0m     validation_filter_split\u001b[39m=\u001b[39;49mvalidation_filter_split,\n\u001b[1;32m   6707\u001b[0m     test_filter_split\u001b[39m=\u001b[39;49mtest_filter_split,\n\u001b[1;32m   6708\u001b[0m     predefined_split_column_name\u001b[39m=\u001b[39;49mpredefined_split_column_name,\n\u001b[1;32m   6709\u001b[0m     timestamp_split_column_name\u001b[39m=\u001b[39;49mtimestamp_split_column_name,\n\u001b[1;32m   6710\u001b[0m     model\u001b[39m=\u001b[39;49mmanaged_model,\n\u001b[1;32m   6711\u001b[0m     model_id\u001b[39m=\u001b[39;49mmodel_id,\n\u001b[1;32m   6712\u001b[0m     parent_model\u001b[39m=\u001b[39;49mparent_model,\n\u001b[1;32m   6713\u001b[0m     is_default_version\u001b[39m=\u001b[39;49mis_default_version,\n\u001b[1;32m   6714\u001b[0m     model_version_aliases\u001b[39m=\u001b[39;49mmodel_version_aliases,\n\u001b[1;32m   6715\u001b[0m     model_version_description\u001b[39m=\u001b[39;49mmodel_version_description,\n\u001b[1;32m   6716\u001b[0m     gcs_destination_uri_prefix\u001b[39m=\u001b[39;49mbase_output_dir,\n\u001b[1;32m   6717\u001b[0m     bigquery_destination\u001b[39m=\u001b[39;49mbigquery_destination,\n\u001b[1;32m   6718\u001b[0m     create_request_timeout\u001b[39m=\u001b[39;49mcreate_request_timeout,\n\u001b[1;32m   6719\u001b[0m )\n\u001b[1;32m   6721\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/works/catchsecu/gcp_ai/gcp-env/lib/python3.8/site-packages/google/cloud/aiplatform/training_jobs.py:824\u001b[0m, in \u001b[0;36m_TrainingJob._run_job\u001b[0;34m(self, training_task_definition, training_task_inputs, dataset, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, annotation_schema_uri, model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, gcs_destination_uri_prefix, bigquery_destination, create_request_timeout)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gca_resource \u001b[39m=\u001b[39m training_pipeline\n\u001b[1;32m    822\u001b[0m _LOGGER\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mView Training:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dashboard_uri())\n\u001b[0;32m--> 824\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_model()\n\u001b[1;32m    826\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     _LOGGER\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    828\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining did not produce a Managed Model returning None. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    829\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_upload_fail_string\n\u001b[1;32m    830\u001b[0m     )\n",
      "File \u001b[0;32m~/works/catchsecu/gcp_ai/gcp-env/lib/python3.8/site-packages/google/cloud/aiplatform/training_jobs.py:911\u001b[0m, in \u001b[0;36m_TrainingJob._get_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_model\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[models\u001b[39m.\u001b[39mModel]:\n\u001b[1;32m    902\u001b[0m     \u001b[39m\"\"\"Helper method to get and instantiate the Model to Upload.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[39m        RuntimeError: If Training failed.\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_block_until_complete()\n\u001b[1;32m    913\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_failed:\n\u001b[1;32m    914\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    915\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Pipeline \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresource_name\u001b[39m}\u001b[39;00m\u001b[39m failed. No model available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    916\u001b[0m         )\n",
      "File \u001b[0;32m~/works/catchsecu/gcp_ai/gcp-env/lib/python3.8/site-packages/google/cloud/aiplatform/training_jobs.py:954\u001b[0m, in \u001b[0;36m_TrainingJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_callback()\n\u001b[1;32m    952\u001b[0m     time\u001b[39m.\u001b[39msleep(_JOB_WAIT_TIME)\n\u001b[0;32m--> 954\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_failure()\n\u001b[1;32m    956\u001b[0m _LOGGER\u001b[39m.\u001b[39mlog_action_completed_against_resource(\u001b[39m\"\u001b[39m\u001b[39mrun\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m)\n\u001b[1;32m    958\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gca_resource\u001b[39m.\u001b[39mmodel_to_upload \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_failed:\n",
      "File \u001b[0;32m~/works/catchsecu/gcp_ai/gcp-env/lib/python3.8/site-packages/google/cloud/aiplatform/training_jobs.py:971\u001b[0m, in \u001b[0;36m_TrainingJob._raise_failure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[39m\"\"\"Helper method to raise failure if TrainingPipeline fails.\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \n\u001b[1;32m    966\u001b[0m \u001b[39mRaises:\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[39m    RuntimeError: If training failed.\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gca_resource\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mcode \u001b[39m!=\u001b[39m code_pb2\u001b[39m.\u001b[39mOK:\n\u001b[0;32m--> 971\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTraining failed with:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gca_resource\u001b[39m.\u001b[39merror)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Training failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=1001227203935&resource=ml_job%2Fjob_id%2F7495461063941423104&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%227495461063941423104%22\"\n"
     ]
    }
   ],
   "source": [
    "model = job.run(\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_MACHINE_TYPE,\n",
    "    accelerator_type=TRAIN_ACCELERATOR_TYPE,\n",
    "    accelerator_count=TRAIN_ACCELERATOR_COUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94d93722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing items under project airy-runway-344101, across all locations.\n",
      "\n",
      "                                                                                 ARTIFACT_REGISTRY\n",
      "REPOSITORY               FORMAT  MODE                 DESCRIPTION                   LOCATION         LABELS  ENCRYPTION          CREATE_TIME          UPDATE_TIME          SIZE (MB)\n",
      "cloud-run-source-deploy  DOCKER  STANDARD_REPOSITORY  Cloud Run Source Deployments  asia-northeast3          Google-managed key  2022-12-08T12:13:57  2022-12-08T12:26:05  1948.728\n"
     ]
    }
   ],
   "source": [
    "# Create the repository in Artifact registry\n",
    "! gcloud artifacts repositories create {APP_NAME} --repository-format=docker --location={REGION} --description=\"Docker repository\"\n",
    "\n",
    "# List all repositories and check your repository\n",
    "! gcloud artifacts repositories list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "843c1ec7e3dd18e461910fbce72982ca8bfca354c7edcf8c00da4737edba7756"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
